# Neural Machine Translation

- Inspired by work of Yunjey Choi (https://github.com/yunjey/deeplearning-papers#deeplearning-papers-2016119-)

## Paper lists

- Sutskever et al. Sequence-to-sequence learning with neural networks
[[paper]](http://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf) 
[[notes]](https://github.com/mjc92/studies/blob/master/notes/Sequence_to_sequence_learning_with_neural_networks.md) 
    - End-to-end approach to sequence learning by using a multilayer LSTM to map input sequence to a fixed vector,
  and another LSTM to decode the fixed vector into a target sequence

- Bahdanau et al. Neural machine translation by jointly learning to align and translate
[[paper]](https://arxiv.org/pdf/1409.0473v7.pdf) 
[[notes]](https://github.com/mjc92/studies/blob/master/notes/Neural_Machine_translation_by_Jointly_Learning_to_Align_and_Translate.md) 
  - Attentional Neural Machine Translation
    - first introduces term "attention"
    - uses bidirectional RNN
  
- Chung et al. Fully character-level neural machine translation without explicit segmentation
[[paper]](https://arxiv.org/pdf/1603.06147v4.pdf)
[[notes]](https://github.com/mjc92/studies/blob/master/notes/Fully_Character-level_Neural_Machine_Translation_without_Explicit_Segmentation.md)
  - Displays character-to-character NMT model without explicit segmentation

